{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11959,"sourceType":"datasetVersion","datasetId":8544}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color: gainsboro; font-size:130%; text-align:left\">\n\n<h1 align=\"left\"><font color=#0a1f89>Objectives:</font></h1>    \n    \n    \n- Download and extract Food 101 dataset.\n- Understand dataset structure and files.\n- Visualize random image from each of the 101 classes.\n- Split the image data into train and test using train.txt and test.txt.\n- Create a subset of data with few classes(3) - train_mini and test_mini for experimenting.\n- Fine tune Inception Pretrained model using Food 101 dataset.\n- Visualize accuracy and loss plots.\n- Predicting classes for new images from internet.\n- Scale up and fine tune Inceptionv3 model with 11 classes of data.\n- Model Explainability.\n- Summary of the things I tried.\n- Further improvements.\n- Feedback.","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Libraries\"></a>\n# <b><span style='color:black'>Step 1.1 |</span><span style='color:#742d0c '> Importing Libraries</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf  # Import TensorFlow library for deep learning tasks\nimport matplotlib.image as img  # Import matplotlib for image reading\nimport numpy as np  # Import NumPy for numerical operations\nfrom collections import defaultdict  # Import defaultdict for creating dictionaries with default values\nimport collections  # Import collections module for collection data types\nfrom shutil import copy  # Import shutil for high-level file operations\nfrom shutil import copytree, rmtree  # Import shutil for directory copying and removal\nimport tensorflow.keras.backend as K  # Import Keras backend functions\nfrom tensorflow.keras.models import load_model  # Import Keras function for loading pre-trained models\nfrom tensorflow.keras.preprocessing import image  # Import Keras for image preprocessing\nimport matplotlib.pyplot as plt  # Import matplotlib for visualization\nimport os  # Import os module for operating system functions\nimport random  # Import random module for generating random numbers\nimport cv2  # Import OpenCV for image processing\nfrom tensorflow.keras import regularizers  # Import regularizers for regularization techniques\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3  # Import pre-trained InceptionV3 model\nfrom tensorflow.keras.models import Sequential, Model  # Import Sequential and Model for building neural network models\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten  # Import layers for building neural network architectures\nfrom tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D  # Import layers for building convolutional neural networks\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator  # Import ImageDataGenerator for data augmentation\nfrom tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger  # Import callbacks for model saving and logging\nfrom tensorflow.keras.optimizers import SGD  # Import SGD optimizer for training models\nfrom tensorflow.keras.regularizers import l2  # Import L2 regularization\nfrom tensorflow import keras  # Import Keras for deep learning tasks\nfrom tensorflow.keras import models  # Import Keras for building neural network models\nimport zipfile\nimport os\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:12.226350Z","iopub.execute_input":"2024-07-16T18:07:12.227246Z","iopub.status.idle":"2024-07-16T18:07:25.130350Z","shell.execute_reply.started":"2024-07-16T18:07:12.227210Z","shell.execute_reply":"2024-07-16T18:07:25.129537Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-16 18:07:13.931180: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-16 18:07:13.931295: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-16 18:07:14.062898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"GPU\"></a>\n# <b><span style='color:black'>Step 1.2 |</span><span style='color:#742d0c '> Checking State of GPU</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Check TensorFlow version\nprint(tf.__version__)\n\n# Check GPU device name\nprint(tf.test.gpu_device_name())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:25.132102Z","iopub.execute_input":"2024-07-16T18:07:25.132861Z","iopub.status.idle":"2024-07-16T18:07:25.418240Z","shell.execute_reply.started":"2024-07-16T18:07:25.132826Z","shell.execute_reply":"2024-07-16T18:07:25.417152Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"2.15.0\n/device:GPU:0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"Directory\"></a>\n# <b><span style='color:black'>Step 1.3 |</span><span style='color:#742d0c '> Changing Directory</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/input/food-101/","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:25.419968Z","iopub.execute_input":"2024-07-16T18:07:25.420620Z","iopub.status.idle":"2024-07-16T18:07:25.449327Z","shell.execute_reply.started":"2024-07-16T18:07:25.420584Z","shell.execute_reply":"2024-07-16T18:07:25.448393Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/food-101\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"DownloadingData\"></a>\n# <b><span style='color:black'>Step 1.4 |</span><span style='color:#742d0c '> Downloading and Extracting the Dataset</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def get_data_extract():\n    \"\"\"\n    Check if the dataset exists, and if not, download and extract it.\n    \"\"\"\n    if \"food-101\" in os.listdir():\n        print(\"Dataset already exists\")\n    else:\n        print(\"Downloading the data...\")\n        !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\n        print(\"Dataset downloaded!\")\n        print(\"Extracting data..\")\n        !tar xzvf food-101.tar.gz\n        print(\"Extraction done!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:25.451510Z","iopub.execute_input":"2024-07-16T18:07:25.451811Z","iopub.status.idle":"2024-07-16T18:07:25.461944Z","shell.execute_reply.started":"2024-07-16T18:07:25.451788Z","shell.execute_reply":"2024-07-16T18:07:25.461257Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Download data and extract it to folder\n# Uncomment this below line if you are on Colab\n\nget_data_extract()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:25.462866Z","iopub.execute_input":"2024-07-16T18:07:25.463109Z","iopub.status.idle":"2024-07-16T18:07:25.476644Z","shell.execute_reply.started":"2024-07-16T18:07:25.463085Z","shell.execute_reply":"2024-07-16T18:07:25.475734Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Dataset already exists\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id=\"DEV\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 2 | Data Exploration and Verification</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Check the extracted dataset folder\n!ls food-101/","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:25.477718Z","iopub.execute_input":"2024-07-16T18:07:25.478068Z","iopub.status.idle":"2024-07-16T18:07:26.503237Z","shell.execute_reply.started":"2024-07-16T18:07:25.478036Z","shell.execute_reply":"2024-07-16T18:07:26.502223Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"__MACOSX  food-101\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n- The dataset being used is **[Food 101](https://www.vision.ee.ethz.ch/datasets_extra/food-101/)**\n- This dataset has 101000 images in total. It's a food dataset with 101 categories(multiclass)\n- Each type of food has 750 training samples and 250 test samples\n- Note found on the webpage of the dataset :  \n- On purpose, the training images were not cleaned, and thus still contain some amount of noise. This comes mostly in the form of intense colors and sometimes wrong labels. \n- All images were rescaled to have a maximum side length of 512 pixels.\n- The entire dataset is 5GB in size","metadata":{}},{"cell_type":"markdown","source":"<a id=\"UnderstandingData\"></a>\n# <b><span style='color:black'>Step 2.1 |</span><span style='color:#742d0c '> Understanding the Structure of Image Data</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"os.listdir('food-101/images')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:26.504671Z","iopub.execute_input":"2024-07-16T18:07:26.504979Z","iopub.status.idle":"2024-07-16T18:07:27.196147Z","shell.execute_reply.started":"2024-07-16T18:07:26.504949Z","shell.execute_reply":"2024-07-16T18:07:27.190023Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfood-101/images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'food-101/images'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'food-101/images'","output_type":"error"}]},{"cell_type":"markdown","source":"<a id=\"MetaData\"></a>\n# <b><span style='color:black'>Step 2.2 |</span><span style='color:#742d0c '> Understanding the Metadata or Additional Information</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\nUnderstanding the Metadata or Additional Information","metadata":{}},{"cell_type":"code","source":"os.listdir('food-101/meta')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.196994Z","iopub.status.idle":"2024-07-16T18:07:27.197350Z","shell.execute_reply.started":"2024-07-16T18:07:27.197176Z","shell.execute_reply":"2024-07-16T18:07:27.197193Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head food-101/meta/train.txt\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.198627Z","iopub.status.idle":"2024-07-16T18:07:27.198930Z","shell.execute_reply.started":"2024-07-16T18:07:27.198779Z","shell.execute_reply":"2024-07-16T18:07:27.198792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head food-101/meta/classes.txt","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.199821Z","iopub.status.idle":"2024-07-16T18:07:27.200116Z","shell.execute_reply.started":"2024-07-16T18:07:27.199967Z","shell.execute_reply":"2024-07-16T18:07:27.199980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DataVisual\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 3 | Data Visualization</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Define the number of rows and columns for the subplot grid\nrows = 17\ncols = 6\n\n# Create a subplot grid with specified size\nfig, ax = plt.subplots(rows, cols, figsize=(25,25))\n\n# Set the title of the plot\nfig.suptitle(\"Showing one random image from each class\", y=1.05, fontsize=24)\n\n# Define the directory containing the image data\ndata_dir = \"food-101/images/\"\n\n# Get a sorted list of food class names\nfoods_sorted = sorted(os.listdir(data_dir))\n\n# Initialize food_id variable\nfood_id = 0\n\n# Loop through rows and columns to display images\nfor i in range(rows):\n    for j in range(cols):\n        try:\n            food_selected = foods_sorted[food_id] \n            food_id += 1\n        except:\n            break\n        if food_selected == '.DS_Store':\n            continue\n        # Get a list of images for the current food class\n        food_selected_images = os.listdir(os.path.join(data_dir, food_selected))\n        # Select a random image from the list\n        food_selected_random = np.random.choice(food_selected_images)\n        # Read and display the image\n        img = plt.imread(os.path.join(data_dir, food_selected, food_selected_random))\n        ax[i][j].imshow(img)\n        ax[i][j].set_title(food_selected, pad=10)  # Set the title of the subplot\n        \n# Remove x and y ticks from all subplots\nplt.setp(ax, xticks=[], yticks=[])\n\n# Adjust the layout of subplots to fit the figure\nplt.tight_layout()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.201703Z","iopub.status.idle":"2024-07-16T18:07:27.202008Z","shell.execute_reply.started":"2024-07-16T18:07:27.201858Z","shell.execute_reply":"2024-07-16T18:07:27.201871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DataProcessing\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 4 | Data Preprocessing</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"Splitting\"></a>\n# <b><span style='color:black'>Step 4.1 |</span><span style='color:#742d0c '> Data Splitting for Training and Test</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Helper method to split dataset into train and test folders\ndef prepare_data(filepath, src, dest):\n    # Create a dictionary to store image paths for each class\n    classes_images = defaultdict(list)\n    \n    # Read the filepath and extract image paths\n    with open(filepath, 'r') as txt:\n        paths = [read.strip() for read in txt.readlines()]\n        for p in paths:\n            food = p.split('/')\n            classes_images[food[0]].append(food[1] + '.jpg')\n\n    # Iterate over classes and copy images to destination folder\n    for food in classes_images.keys():\n        print(\"\\nCopying images into \", food)\n        if not os.path.exists(os.path.join(dest, food)):\n            os.makedirs(os.path.join(dest, food))\n        for i in classes_images[food]:\n            copy(os.path.join(src, food, i), os.path.join(dest, food, i))\n    print(\"Copying Done!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.203147Z","iopub.status.idle":"2024-07-16T18:07:27.203474Z","shell.execute_reply.started":"2024-07-16T18:07:27.203307Z","shell.execute_reply":"2024-07-16T18:07:27.203321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TrainData\"></a>\n# <b><span style='color:black'>Step 4.2 |</span><span style='color:#742d0c '> Prepares the Train Dataset </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Change current directory to the root directory\n%cd /\n\n# Print message indicating the start of creating train data\nprint(\"Creating train data...\")\n\n# Call prepare_data function to copy images from train.txt to train directory\nprepare_data('/kaggle/input/food-101/food-101/meta/train.txt', '/kaggle/input/food-101/food-101/images', 'train')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.204865Z","iopub.status.idle":"2024-07-16T18:07:27.205191Z","shell.execute_reply.started":"2024-07-16T18:07:27.205031Z","shell.execute_reply":"2024-07-16T18:07:27.205045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TestData\"></a>\n# <b><span style='color:black'>Step 4.3 |</span><span style='color:#742d0c '> Creating the Test Data </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Print message indicating the start of creating test data\nprint(\"Creating test data...\")\n\n# Call prepare_data function to copy images from test.txt to test directory\nprepare_data('/kaggle/input/food-101/food-101/meta/test.txt', '/kaggle/input/food-101/food-101/images', 'test')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.206632Z","iopub.status.idle":"2024-07-16T18:07:27.207078Z","shell.execute_reply.started":"2024-07-16T18:07:27.206843Z","shell.execute_reply":"2024-07-16T18:07:27.206863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TrainCount\"></a>\n# <b><span style='color:black'>Step 4.4 |</span><span style='color:#742d0c '> Counting the Files and Directories in \"Train\" Folder </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Print message indicating the total number of samples in the train folder\nprint(\"Total number of samples in train folder\")\n\n# Execute the find command to search for files and directories in the train folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find train -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.208328Z","iopub.status.idle":"2024-07-16T18:07:27.208680Z","shell.execute_reply.started":"2024-07-16T18:07:27.208506Z","shell.execute_reply":"2024-07-16T18:07:27.208526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TestCount\"></a>\n# <b><span style='color:black'>Step 4.5 |</span><span style='color:#742d0c '> Counting the Files and Directories in \"Test\" Folder </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Print message indicating the total number of samples in the test folder\nprint(\"Total number of samples in test folder\")\n\n# Execute the find command to search for files and directories in the test folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find test -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.209741Z","iopub.status.idle":"2024-07-16T18:07:27.210066Z","shell.execute_reply.started":"2024-07-16T18:07:27.209897Z","shell.execute_reply":"2024-07-16T18:07:27.209911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n- We now have train and test data ready  \n- But to experiment and try different architectures, working on the whole data with 101 classes takes a lot of time and computation  \n- To proceed with further experiments, I am creating train_min and test_mini, limiting the dataset to 3 classes  \n- Since the original problem is multiclass classification which makes key aspects of architectural decisions different from that of binary classification, choosing 3 classes is a good start instead of 2","metadata":{}},{"cell_type":"markdown","source":"<a id=\"DS_Store\"></a>\n# <b><span style='color:black'>Step 4.6 |</span><span style='color:#742d0c '> Removing the .DS_Store Entry </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# List of all 101 types of foods(sorted alphabetically)\ndel foods_sorted[0] # remove .DS_Store from the list","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.211388Z","iopub.status.idle":"2024-07-16T18:07:27.211747Z","shell.execute_reply.started":"2024-07-16T18:07:27.211584Z","shell.execute_reply":"2024-07-16T18:07:27.211598Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(foods_sorted)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.213168Z","iopub.status.idle":"2024-07-16T18:07:27.213653Z","shell.execute_reply.started":"2024-07-16T18:07:27.213374Z","shell.execute_reply":"2024-07-16T18:07:27.213395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"CreaingSubset\"></a>\n# <b><span style='color:black'>Step 4.7 |</span><span style='color:#742d0c '> Creaing Subset </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Helper method to create train_mini and test_mini data samples\ndef dataset_mini(food_list, src, dest):\n    # Check if the destination directory exists\n    if os.path.exists(dest):\n        # If it exists, remove it to ensure a clean slate\n        rmtree(dest)  # Removing dataset_mini (if it already exists) folders\n    # Create the destination directory\n    os.makedirs(dest)\n    \n    # Iterate over each food item in the provided list\n    for food_item in food_list:\n        print(\"Copying images into\", food_item)\n        # Recursively copy the images from the source directory to the destination directory for each food item\n        copytree(os.path.join(src, food_item), os.path.join(dest, food_item))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.215842Z","iopub.status.idle":"2024-07-16T18:07:27.216222Z","shell.execute_reply.started":"2024-07-16T18:07:27.216035Z","shell.execute_reply":"2024-07-16T18:07:27.216051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# List of food items for creating mini datasets\nfood_list = ['apple_pie', 'pizza', 'omelette']\n\n# Source and destination directories for train and test datasets\nsrc_train = 'train'\ndest_train = 'train_mini'\nsrc_test = 'test'\ndest_test = 'test_mini'\n\n# Create train_mini dataset\ndataset_mini(food_list, src_train, dest_train)\n\n# Create test_mini dataset\ndataset_mini(food_list, src_test, dest_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.217902Z","iopub.status.idle":"2024-07-16T18:07:27.218287Z","shell.execute_reply.started":"2024-07-16T18:07:27.218097Z","shell.execute_reply":"2024-07-16T18:07:27.218114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the creation of the train data folder with new classes\nprint(\"Creating train data folder with new classes\")\n\n# Create train_mini dataset with specified food classes\ndataset_mini(food_list, src_train, dest_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.219815Z","iopub.status.idle":"2024-07-16T18:07:27.220195Z","shell.execute_reply.started":"2024-07-16T18:07:27.220007Z","shell.execute_reply":"2024-07-16T18:07:27.220023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the total number of samples in the train folder\nprint(\"Total number of samples in train folder\")\n\n# Execute the find command to search for files and directories in the train_mini folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find train_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.221336Z","iopub.status.idle":"2024-07-16T18:07:27.221891Z","shell.execute_reply.started":"2024-07-16T18:07:27.221609Z","shell.execute_reply":"2024-07-16T18:07:27.221631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the creation of the test data folder with new classes\nprint(\"Creating test data folder with new classes\")\n\n# Create test_mini dataset with specified food classes\ndataset_mini(food_list, src_test, dest_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.223353Z","iopub.status.idle":"2024-07-16T18:07:27.223763Z","shell.execute_reply.started":"2024-07-16T18:07:27.223572Z","shell.execute_reply":"2024-07-16T18:07:27.223589Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print message indicating the total number of samples in the test folder\nprint(\"Total number of samples in test folder\")\n\n# Execute the find command to search for files and directories in the test_mini folder\n# -type d: Search for directories\n# -or: Logical OR operator\n# -type f: Search for regular files\n# -printf '.': Print a single character for each file or directory found\n# wc -c: Count the number of characters (which corresponds to the number of files and directories)\n!find test_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.225908Z","iopub.status.idle":"2024-07-16T18:07:27.226258Z","shell.execute_reply.started":"2024-07-16T18:07:27.226083Z","shell.execute_reply":"2024-07-16T18:07:27.226098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"TrainingNeuralNetwork\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 5 | Training Neural Network Model for Image Classification</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- Keras and other Deep Learning libraries provide pretrained models  \n- These are deep neural networks with efficient architectures(like VGG,Inception,ResNet) that are already trained on datasets like ImageNet  \n- Using these pretrained models, we can use the already learned weights and add few layers on top to finetune the model to our new data  \n- This helps in faster convergance and saves time and computation when compared to models trained from scratch","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n- We currently have a subset of dataset with 3 classes - samosa, pizza and omelette  \n- Use the below code to finetune Inceptionv3 pretrained model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"FinetuneInception1\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:170%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Fine tune Inception Pretrained model using Food 101 dataset</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Clear Keras session to release resources\nK.clear_session()\n\n# Number of classes in the dataset\nn_classes = 3\n\n# Image dimensions\nimg_width, img_height = 299, 299\n\n# Directories for training and validation data\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\n\n# Number of samples in the training and validation sets\nnb_train_samples = 2250  # Number of training samples\nnb_validation_samples = 750  # Number of validation samples\n\n# Batch size for training\nbatch_size = 16\n\n# Data augmentation and normalization for training images\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,  # Normalize pixel values to the range [0,1]\n    shear_range=0.2,   # Shear transformation\n    zoom_range=0.2,    # Random zoom\n    horizontal_flip=True)  # Horizontal flip\n\n# Normalization for validation images\ntest_datagen = ImageDataGenerator(rescale=1. / 255)  # Normalize pixel values to the range [0,1]\n\n# Generate batches of training data\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,  # Path to the training data directory\n    target_size=(img_height, img_width),  # Resize images to match the input size of the model\n    batch_size=batch_size,  # Number of samples per batch\n    class_mode='categorical')  # Use categorical labels for multi-class classification\n\n# Generate batches of validation data\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,  # Path to the validation data directory\n    target_size=(img_height, img_width),  # Resize images to match the input size of the model\n    batch_size=batch_size,  # Number of samples per batch\n    class_mode='categorical')  # Use categorical labels for multi-class classification\n\n# Load the pre-trained InceptionV3 model without the top layers\ninception = InceptionV3(weights='imagenet', include_top=False)\n\n# Add custom top layers for fine-tuning\nx = inception.output  # Output tensor of the InceptionV3 model\nx = GlobalAveragePooling2D()(x)  # Global average pooling layer\nx = Dense(128, activation='relu')(x)  # Fully connected layer with ReLU activation\nx = Dropout(0.2)(x)  # Dropout layer for regularization\n\n# Predictions layer with softmax activation for class probabilities\npredictions = Dense(3, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\n# Create the final model with InceptionV3 as the base and custom top layers\nmodel = Model(inputs=inception.input, outputs=predictions)\n\n# Compile the model with SGD optimizer, categorical cross-entropy loss, and accuracy metric\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define callbacks for saving the best model and logging training history\ncheckpointer = ModelCheckpoint(filepath='best_model_3class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_3class.log')\n\n# Train the model using the training and validation generators\nhistory = model.fit_generator(\n    train_generator,\n    steps_per_epoch=nb_train_samples // batch_size,  # Number of batches per epoch\n    validation_data=validation_generator,\n    validation_steps=nb_validation_samples // batch_size,  # Number of validation batches per epoch\n    epochs=30,  # Number of training epochs\n    verbose=1,  # Verbosity mode (0=silent, 1=progress bar, 2=one line per epoch)\n    callbacks=[csv_logger, checkpointer])  # List of callbacks for training\n\n# Save the trained model\nmodel.save('model_trained_3class.hdf5')  # Save the model to HDF5 file format\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.228048Z","iopub.status.idle":"2024-07-16T18:07:27.228459Z","shell.execute_reply.started":"2024-07-16T18:07:27.228247Z","shell.execute_reply":"2024-07-16T18:07:27.228264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"IndicesMapping\"></a>\n# <b><span style='color:black'>Step 5.1 |</span><span style='color:#742d0c '> Obtaining the Class Indices Mapping </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Get the class indices mapping for the labels in the training data generator\nclass_map_3 = train_generator.class_indices\n\n# Display the class indices mapping\nclass_map_3\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.229516Z","iopub.status.idle":"2024-07-16T18:07:27.229905Z","shell.execute_reply.started":"2024-07-16T18:07:27.229712Z","shell.execute_reply":"2024-07-16T18:07:27.229728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"VisualizingTrain/Validation\"></a>\n# <b><span style='color:black'>Step 5.2 |</span><span style='color:#742d0c '> Visualizing the Training and Validation Accuracy </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def plot_accuracy(history, title):\n    \"\"\"\n    Plot training and validation accuracy over epochs.\n    \n    Args:\n    - history: Training history obtained from model training\n    - title: Title of the plot\n    \n    Returns:\n    - None\n    \"\"\"\n    plt.title(title)\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train_accuracy', 'validation_accuracy'], loc='best')\n    plt.show()\n\ndef plot_loss(history, title):\n    \"\"\"\n    Plot training and validation loss over epochs.\n    \n    Args:\n    - history: Training history obtained from model training\n    - title: Title of the plot\n    \n    Returns:\n    - None\n    \"\"\"\n    plt.title(title)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train_loss', 'validation_loss'], loc='best')\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.231409Z","iopub.status.idle":"2024-07-16T18:07:27.231944Z","shell.execute_reply.started":"2024-07-16T18:07:27.231680Z","shell.execute_reply":"2024-07-16T18:07:27.231702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ModelEvaluation\"></a>\n# <b><span style='color:black'>Step 5.3 |</span><span style='color:#742d0c '> Model Evaluation </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"plot_accuracy(history, 'FOOD101-Inceptionv3')  # Plot training and validation accuracy\nplot_loss(history, 'FOOD101-Inceptionv3')      # Plot training and validation loss\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.238252Z","iopub.status.idle":"2024-07-16T18:07:27.238627Z","shell.execute_reply.started":"2024-07-16T18:07:27.238428Z","shell.execute_reply":"2024-07-16T18:07:27.238442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n   \n- The plots show that the accuracy of the model increased with epochs and the loss has decreased\n- Validation accuracy has been on the higher side than training accuracy for many epochs\nThis could be for several reasons:\n\n-   We used a pretrained model trained on ImageNet which contains data from a variety of classes\n-   Using dropout can lead to a higher validation accuracy","metadata":{}},{"cell_type":"code","source":"%%time\n# Loading the best saved model to make predictions\nK.clear_session()  # Clear Keras session\nmodel_best = load_model('best_model_3class.hdf5', compile=False)  # Load the best saved model\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.240008Z","iopub.status.idle":"2024-07-16T18:07:27.240339Z","shell.execute_reply.started":"2024-07-16T18:07:27.240177Z","shell.execute_reply":"2024-07-16T18:07:27.240191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n   \n- Setting compile=False and clearing the session leads to faster loading of the saved model\n- Withouth the above addiitons, model loading was taking more than a minute!","metadata":{}},{"cell_type":"markdown","source":"<a id=\"PredictingClassLabel\"></a>\n# <b><span style='color:black'>Step 5.4 |</span><span style='color:#742d0c '> Predicting Class Label </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def predict_class(model, images, show=True):\n    \"\"\"\n    Predict the class label for each image in the given list of image paths.\n    \n    Args:\n    - model: Trained model for making predictions\n    - images: List of image paths\n    - show: Boolean flag to control image display\n    \n    Returns:\n    - None\n    \"\"\"\n    for img in images:\n        img = image.load_img(img, target_size=(299, 299))  # Load image and resize to model's input size\n        img = image.img_to_array(img)                     # Convert image to numpy array\n        img = np.expand_dims(img, axis=0)                 # Add batch dimension\n        img /= 255.                                       # Normalize pixel values\n\n        pred = model.predict(img)                         # Make prediction\n        index = np.argmax(pred)                           # Get the index of the class with the highest probability\n        food_list.sort()                                  # Sort the list of food items\n        pred_value = food_list[index]                     # Get the predicted class label\n        \n        if show:\n            plt.imshow(img[0])                           # Display the image\n            plt.axis('off')\n            plt.title(pred_value)                        # Set title as the predicted class label\n            plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.241960Z","iopub.status.idle":"2024-07-16T18:07:27.242264Z","shell.execute_reply.started":"2024-07-16T18:07:27.242113Z","shell.execute_reply":"2024-07-16T18:07:27.242126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DownloadImages\"></a>\n# <b><span style='color:black'>Step 5.5 |</span><span style='color:#742d0c '> Downloading Images from the Internet</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Downloading images from internet using the URLs\n!wget -O samosa.jpg http://veggiefoodrecipes.com/wp-content/uploads/2016/05/lentil-samosa-recipe-01.jpg\n!wget -O applepie.jpg https://acleanbake.com/wp-content/uploads/2017/10/Paleo-Apple-Pie-with-Crumb-Topping-gluten-free-grain-free-dairy-free-15.jpg\n!wget -O pizza.jpg http://104.130.3.186/assets/itemimages/400/400/3/default_9b4106b8f65359684b3836096b4524c8_pizza%20dreamstimesmall_94940296.jpg\n!wget -O omelette.jpg https://www.incredibleegg.org/wp-content/uploads/basic-french-omelet-930x550.jpg\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.243191Z","iopub.status.idle":"2024-07-16T18:07:27.243529Z","shell.execute_reply.started":"2024-07-16T18:07:27.243341Z","shell.execute_reply":"2024-07-16T18:07:27.243354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a list of downloaded images\nimages = ['applepie.jpg', 'pizza.jpg', 'omelette.jpg']\n\n# Test the trained model\npredict_class(model_best, images, True)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.244907Z","iopub.status.idle":"2024-07-16T18:07:27.245210Z","shell.execute_reply.started":"2024-07-16T18:07:27.245057Z","shell.execute_reply":"2024-07-16T18:07:27.245070Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#ed2323>Yes!!! The model got them all right!!</font></h2>\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"FinetuneInception\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:170%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Fine tune Inceptionv3 model with 11 classes of data</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- We trained a model on 3 classes and tested it using new data\n- The model was able to predict the classes of all three test images correctly\n- Will it be able to perform at the same level of accuracy for more classes?\n- FOOD-101 dataset has 101 classes of data\n- Even with fine tuning using a pre-trained model, each epoch was taking more than an hour when all 101 classes of data is used(tried this on both Colab and on a Deep Learning VM instance with P100 GPU on GCP)\n- But to check how the model performs when more classes are included, I'm using the same model to fine tune and train on 11 randomly chosen classes","metadata":{}},{"cell_type":"code","source":"def pick_n_random_classes(n):\n    \"\"\"\n    Select n random food classes from the sorted list of food items.\n    \n    Args:\n    - n: Number of random food classes to select\n    \n    Returns:\n    - List of n randomly selected food classes\n    \"\"\"\n    food_list = []\n    random_food_indices = random.sample(range(len(foods_sorted)), n)  # Sample n random indices\n    for i in random_food_indices:\n        food_list.append(foods_sorted[i])  # Retrieve corresponding food items\n    food_list.sort()  # Sort the list of randomly selected food classes\n    return food_list\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.246323Z","iopub.status.idle":"2024-07-16T18:07:27.246694Z","shell.execute_reply.started":"2024-07-16T18:07:27.246525Z","shell.execute_reply":"2024-07-16T18:07:27.246540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n = 11  # Number of random food classes to select\nfood_list = pick_n_random_classes(n)  # Select n random food classes\nfood_list = ['apple_pie', 'beef_carpaccio', 'bibimbap', 'cup_cakes', 'foie_gras', 'french_fries', 'garlic_bread', 'pizza', 'spring_rolls', 'spaghetti_carbonara', 'strawberry_shortcake']\nprint(\"These are the randomly picked food classes we will be training the model on...\\n\", food_list)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.247831Z","iopub.status.idle":"2024-07-16T18:07:27.248135Z","shell.execute_reply.started":"2024-07-16T18:07:27.247984Z","shell.execute_reply":"2024-07-16T18:07:27.247997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a message indicating the start of the process\nprint(\"Creating training data folder with new classes...\")\n\n# Call the dataset_mini function to create a new data subset with the selected food classes for training\ndataset_mini(food_list, src_train, dest_train)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.249734Z","iopub.status.idle":"2024-07-16T18:07:27.250193Z","shell.execute_reply.started":"2024-07-16T18:07:27.249941Z","shell.execute_reply":"2024-07-16T18:07:27.249959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print the total number of samples in the train folder\nprint(\"Total number of samples in train folder\")\n\n# Count the number of files and directories in the train_mini folder and print the count\n!find train_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.251638Z","iopub.status.idle":"2024-07-16T18:07:27.252067Z","shell.execute_reply.started":"2024-07-16T18:07:27.251847Z","shell.execute_reply":"2024-07-16T18:07:27.251866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a message indicating the start of the process\nprint(\"Creating test data folder with new classes\")\n\n# Call the dataset_mini function to create a new data subset with the selected food classes for testing\ndataset_mini(food_list, src_test, dest_test)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.253087Z","iopub.status.idle":"2024-07-16T18:07:27.253393Z","shell.execute_reply.started":"2024-07-16T18:07:27.253236Z","shell.execute_reply":"2024-07-16T18:07:27.253248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Print a message indicating the start of the process\nprint(\"Total number of samples in test folder\")\n\n# Count the number of files and directories in the test_mini folder and print the count\n!find test_mini -type d -or -type f -printf '.' | wc -c\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.254707Z","iopub.status.idle":"2024-07-16T18:07:27.255032Z","shell.execute_reply.started":"2024-07-16T18:07:27.254872Z","shell.execute_reply":"2024-07-16T18:07:27.254886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clear any previous sessions to free up memory\nK.clear_session()\n\n# Set the number of classes\nn_classes = n\n\n# Set the dimensions of input images\nimg_width, img_height = 299, 299\n\n# Set the paths for the training and validation data directories\ntrain_data_dir = 'train_mini'\nvalidation_data_dir = 'test_mini'\n\n# Set the number of training and validation samples\nnb_train_samples = 8250\nnb_validation_samples = 2750\n\n# Set the batch size for training\nbatch_size = 16\n\n# Define data augmentation for training images\ntrain_datagen = ImageDataGenerator(\n    rescale=1. / 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\n# Define data augmentation for validation images\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\n# Generate batches of augmented training and validation data\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\n# Load the InceptionV3 model pretrained on ImageNet without the top layer\ninception = InceptionV3(weights='imagenet', include_top=False)\n\n# Add custom top layers for classification\nx = inception.output\nx = GlobalAveragePooling2D()(x)\nx = Dense(128, activation='relu')(x)\nx = Dropout(0.2)(x)\npredictions = Dense(n, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n\n# Create the final model\nmodel = Model(inputs=inception.input, outputs=predictions)\n\n# Compile the model\nmodel.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Define callbacks for model checkpointing and logging\ncheckpointer = ModelCheckpoint(filepath='best_model_11class.hdf5', verbose=1, save_best_only=True)\ncsv_logger = CSVLogger('history_11class.log')\n\n# Train the model\nhistory_11class = model.fit_generator(train_generator,\n                    steps_per_epoch=nb_train_samples // batch_size,\n                    validation_data=validation_generator,\n                    validation_steps=nb_validation_samples // batch_size,\n                    epochs=30,\n                    verbose=1,\n                    callbacks=[csv_logger, checkpointer])\n\n# Save the trained model\nmodel.save('model_trained_11class.hdf5')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.256500Z","iopub.status.idle":"2024-07-16T18:07:27.256815Z","shell.execute_reply.started":"2024-07-16T18:07:27.256659Z","shell.execute_reply":"2024-07-16T18:07:27.256673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get the class indices for the 11 food classes from the training data generator\nclass_map_11 = train_generator.class_indices\nclass_map_11\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.258043Z","iopub.status.idle":"2024-07-16T18:07:27.258343Z","shell.execute_reply.started":"2024-07-16T18:07:27.258193Z","shell.execute_reply":"2024-07-16T18:07:27.258205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot the accuracy over epochs for the training and validation sets\nplot_accuracy(history_11class, 'FOOD101-Inceptionv3')\n\n# Plot the loss over epochs for the training and validation sets\nplot_loss(history_11class, 'FOOD101-Inceptionv3')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.259848Z","iopub.status.idle":"2024-07-16T18:07:27.260175Z","shell.execute_reply.started":"2024-07-16T18:07:27.260015Z","shell.execute_reply":"2024-07-16T18:07:27.260029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- The plots show that the accuracy of the model increased with epochs and the loss has decreased\n- Validation accuracy has been on the higher side than training accuracy for many epochs\n\n   - This could be for several reasons:\n    \n    \n- We used a pretrained model trained on ImageNet which contains data from a variety of classes\n- Using dropout can lead to a higher validation accuracy \n","metadata":{}},{"cell_type":"code","source":"%%time\n# Clear any previous session and load the best saved model for predictions\nK.clear_session()\nmodel_best = load_model('best_model_11class.hdf5', compile=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.261280Z","iopub.status.idle":"2024-07-16T18:07:27.261622Z","shell.execute_reply.started":"2024-07-16T18:07:27.261429Z","shell.execute_reply":"2024-07-16T18:07:27.261442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Downloading images from internet using the URLs\n!wget -O cupcakes.jpg https://www.publicdomainpictures.net/pictures/110000/nahled/halloween-witch-cupcakes.jpg\n!wget -O springrolls.jpg https://upload.wikimedia.org/wikipedia/commons/6/6f/Vietnamese_spring_rolls.jpg\n!wget -O pizza.jpg http://104.130.3.186/assets/itemimages/400/400/3/default_9b4106b8f65359684b3836096b4524c8_pizza%20dreamstimesmall_94940296.jpg\n!wget -O garlicbread.jpg https://c1.staticflickr.com/1/84/262952165_7ba3466108_z.jpg?zz=1\n\n# If you have an image in your local computer and want to try it, uncomment the below code to upload the image files\n\n\n# from google.colab import files\n# image = files.upload()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.263280Z","iopub.status.idle":"2024-07-16T18:07:27.263644Z","shell.execute_reply.started":"2024-07-16T18:07:27.263434Z","shell.execute_reply":"2024-07-16T18:07:27.263448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a list of downloaded images and test the trained model\nimages = []\nimages.append('cupcakes.jpg')\nimages.append('pizza.jpg')\nimages.append('springrolls.jpg')\nimages.append('garlicbread.jpg')\npredict_class(model_best, images, True)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.264727Z","iopub.status.idle":"2024-07-16T18:07:27.265050Z","shell.execute_reply.started":"2024-07-16T18:07:27.264890Z","shell.execute_reply":"2024-07-16T18:07:27.264903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- The model did well even when the number of classes are increased to 11\n- Model training on all 101 classes takes some time\n- It was taking more than an hour for one epoch when the full dataset is used for fine tuning","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ModelExplainability\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 6 | Model Explainability</p>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- Human lives and Technology are blending more and more together\n- The rapid advancements in technology over the past few years can be attributed to how Neural Networks have evolved\n- Neural Networks and Deep Learning are now being used in so many fields and industries - healthcare, finance, retail, automative etc\n- Thanks to the Deep Learning libraries which enable us to develop applications/models with few lines of code, which a decade ago only those with a lot of expertise and research could do\n- All of this calls for the need to understand how neural networks do what they do and how they do it\n- This has led to an active area of research - Neural Network Model Interpretability and Explainability","metadata":{}},{"cell_type":"markdown","source":"![](https://s3-media2.fl.yelpcdn.com/bphoto/7BlRoSOG3AsAWHMPOaG7ng/ls.jpg)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n- Neural Networks learn incrementally\n- How does a neural network know what is in the image and how does it conclude that its a dog?\n- The best analogy to understand the incremental learning of the model here is to think about how we would hand sketch the dog\n- You can't start right away by drawing eyes, nose, snout etc\n- To have any of those dogly features, you need a lot of edges and curves\n- You start with edges/lines, put many of them together\n- Use edges with curves to sketch patterns\n- The patterns with more finer details will help us draw the visible features of a dog like eyes, ears, snout etc\n- Neural networks adopt a very similar process when they are busy detecting what's in the provided data examples","metadata":{}},{"cell_type":"markdown","source":"![](https://images.deepai.org/publication-preview/visualizing-and-understanding-convolutional-networks-page-4-medium.jpg)","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* The above image is taken from the paper - [Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)\n* The image contains the features of a trained model along with the kind of objects they would detect\n* In the first row and first column, we have a grid of edge detecting features in layer 1 and some curve detectors in layer 2 in the 2nd column\n* The last column in 1st row are the kind of objects that get detected using those curvy features\n* With layer three in 2nd row, the model starts looking for patterns with edges and curves\n* The second column in second row contains examples of patterns that are detected in layer 3 of the model\n* With layer 4, the model starts detecting parts of object specific features and in layer 5 the model knows what's in the image","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* Using feature visualization, we can know what a neural network layer and its features are looking for\n* Using attribution, we can understand how the features impact the output and what regions in the image led the model to the generated output","metadata":{}},{"cell_type":"markdown","source":"<a id=\"EvaluateModel\"></a>\n# <p style=\"background-color: #742d0c; font-family:calibri; color:white; font-size:140%; font-family:Verdana; text-align:center; border-radius:15px 50px;\">Step 7 | Evaluating the Model</p>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"markdown","source":"<a id=\"LoadingtheSavedModel\"></a>\n# <b><span style='color:black'>Step 7.1 |</span><span style='color:#742d0c '> Loading the Saved Model and a Test Image</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n\n","metadata":{}},{"cell_type":"code","source":"# Load the saved model trained with 3 classes\nK.clear_session()\nprint(\"Loading the model..\")\nmodel = load_model('best_model_3class.hdf5', compile=False)\nprint(\"Done!\")\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.266324Z","iopub.status.idle":"2024-07-16T18:07:27.266679Z","shell.execute_reply.started":"2024-07-16T18:07:27.266508Z","shell.execute_reply":"2024-07-16T18:07:27.266526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"SummaryModel\"></a>\n# <b><span style='color:black'>Step 7.2 |</span><span style='color:#742d0c '> Summary of the Model </span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.268162Z","iopub.status.idle":"2024-07-16T18:07:27.268509Z","shell.execute_reply.started":"2024-07-16T18:07:27.268324Z","shell.execute_reply":"2024-07-16T18:07:27.268338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"DefiningHelperFunctions\"></a>\n# <b><span style='color:black'>Step 7.3 |</span><span style='color:#742d0c '> Defining Helper Functions</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def deprocess_image(x):\n    # Normalize tensor: center on 0., ensure standard deviation is 0.1\n    x -= x.mean()\n    x /= (x.std() + 1e-5)\n    x *= 0.1\n\n    # Clip values to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # Convert to RGB array\n    x *= 255\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.269757Z","iopub.status.idle":"2024-07-16T18:07:27.270082Z","shell.execute_reply.started":"2024-07-16T18:07:27.269921Z","shell.execute_reply":"2024-07-16T18:07:27.269934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GeneratingPatternFunction\"></a>\n# <b><span style='color:black'>Step 7.4 |</span><span style='color:#742d0c '> Generating Pattern Function\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def generate_pattern(layer_name, filter_index, size=150):\n    # Build a loss function that maximizes the activation\n    # of the nth filter of the layer considered.\n    layer_output = model.get_layer(layer_name).output\n    loss = K.mean(layer_output[:, :, :, filter_index])\n\n    # Compute the gradient of the input picture wrt this loss\n    grads = K.gradients(loss, model.input)[0]\n\n    # Normalization trick: we normalize the gradient\n    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)\n\n    # This function returns the loss and grads given the input picture\n    iterate = K.function([model.input], [loss, grads])\n    \n    # We start from a gray image with some noise\n    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.\n\n    # Run gradient ascent for 40 steps\n    step = 1.\n    for i in range(40):\n        loss_value, grads_value = iterate([input_img_data])\n        input_img_data += grads_value * step\n        \n    img = input_img_data[0]\n    return deprocess_image(img)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.271310Z","iopub.status.idle":"2024-07-16T18:07:27.271655Z","shell.execute_reply.started":"2024-07-16T18:07:27.271464Z","shell.execute_reply":"2024-07-16T18:07:27.271477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GettingActivationsFunction\"></a>\n# <b><span style='color:black'>Step 7.5 |</span><span style='color:#742d0c '> Getting Activations Function\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"def get_activations(img, model_activations):\n    \"\"\"\n    Get activations of a model for a given image.\n\n    Args:\n    img (str): Path to the image file.\n    model_activations (Model): Model object to get activations from.\n\n    Returns:\n    numpy.ndarray: Activations produced by the model.\n    \"\"\"\n    # Load and preprocess the image\n    img = image.load_img(img, target_size=(299, 299))\n    img = image.img_to_array(img)                    \n    img = np.expand_dims(img, axis=0)         \n    img /= 255. \n    # Visualize the image\n    plt.imshow(img[0])\n    plt.show()\n    # Get activations\n    return model_activations.predict(img)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.273334Z","iopub.status.idle":"2024-07-16T18:07:27.273681Z","shell.execute_reply.started":"2024-07-16T18:07:27.273515Z","shell.execute_reply":"2024-07-16T18:07:27.273533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"ShowingActivationsFunction\"></a>\n# <b><span style='color:black'>Step 7.6 |</span><span style='color:#742d0c '> Showing Activations Function\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def show_activations(activations, layer_names):\n    \"\"\"\n    Display feature maps for each layer.\n\n    Args:\n    activations (list of numpy.ndarray): List of activation tensors for each layer.\n    layer_names (list of str): Names of the layers.\n\n    Returns:\n    None\n    \"\"\"\n    images_per_row = 16\n\n    # Loop through each layer\n    for layer_name, layer_activation in zip(layer_names, activations):\n        # Number of features in the feature map\n        n_features = layer_activation.shape[-1]\n\n        # Size of the feature map\n        size = layer_activation.shape[1]\n\n        # Number of columns for visualization\n        n_cols = n_features // images_per_row\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n\n        # Tile each filter into a big horizontal grid\n        for col in range(n_cols):\n            for row in range(images_per_row):\n                channel_image = layer_activation[0, :, :, col * images_per_row + row]\n                # Post-process the feature to make it visually palatable\n                channel_image -= channel_image.mean()\n                channel_image /= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size, row * size : (row + 1) * size] = channel_image\n\n        # Display the grid\n        scale = 1. / size\n        plt.figure(figsize=(scale * display_grid.shape[1], scale * display_grid.shape[0]))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n\n    plt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.274922Z","iopub.status.idle":"2024-07-16T18:07:27.275252Z","shell.execute_reply.started":"2024-07-16T18:07:27.275092Z","shell.execute_reply":"2024-07-16T18:07:27.275105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\nCheck how many layers are in the trained model(this includes the 1st input layer as well)\n","metadata":{}},{"cell_type":"code","source":"len(model.layers)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.276265Z","iopub.status.idle":"2024-07-16T18:07:27.276626Z","shell.execute_reply.started":"2024-07-16T18:07:27.276428Z","shell.execute_reply":"2024-07-16T18:07:27.276442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n> 1. * Can we visualize the outputs of all the layers?\n* Yes, we can. But that gets too tedious\n* So, let's choose a few layers to visualize","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ExtractingIntermediateLayerActivations\"></a>\n# <b><span style='color:black'>Step 7.7 |</span><span style='color:#742d0c '> Extracting Intermediate Layer Activations\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# We start with index 1 instead of 0, as input layer is at index 0\nlayers = [layer.output for layer in model.layers[1:11]]\n# We now initialize a model which takes an input and outputs the above chosen layers\nactivations_output = models.Model(inputs=model.input, outputs=layers)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.278203Z","iopub.status.idle":"2024-07-16T18:07:27.278543Z","shell.execute_reply.started":"2024-07-16T18:07:27.278359Z","shell.execute_reply":"2024-07-16T18:07:27.278371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\nAs seen below, the 10 chosen layers contain 3 convolution, 3 batch normalization, 3 activation and 1 max pooling layers\n","metadata":{}},{"cell_type":"code","source":"layers","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.279822Z","iopub.status.idle":"2024-07-16T18:07:27.280147Z","shell.execute_reply.started":"2024-07-16T18:07:27.279987Z","shell.execute_reply":"2024-07-16T18:07:27.280001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n>  * Get the names of all the selected layers","metadata":{}},{"cell_type":"markdown","source":"<a id=\"ExtractingLayerNamesforIntermediateActivations\"></a>\n# <b><span style='color:black'>Step 7.8 |</span><span style='color:#742d0c '> Extracting Layer Names for Intermediate Activations\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Initialize an empty list to store the names of the layers\nlayer_names = []\n\n# Iterate through the layers of the model starting from index 1 and ending at index 10 (inclusive)\nfor layer in model.layers[1:11]:\n    # Append the name of each layer to the list\n    layer_names.append(layer.name)\n\n# Print the list of layer names\nprint(layer_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.496168Z","iopub.status.idle":"2024-07-16T18:07:27.496663Z","shell.execute_reply.started":"2024-07-16T18:07:27.496407Z","shell.execute_reply":"2024-07-16T18:07:27.496427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \nProvide an input to the model and get the activations of all the 10 chosen layers\n","metadata":{}},{"cell_type":"code","source":"# Define the filename of the image\nfood = 'applepie.jpg'\n\n# Get the activations of the model for the specified image using the defined activations_output model\nactivations = get_activations(food, activations_output)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.498000Z","iopub.status.idle":"2024-07-16T18:07:27.498361Z","shell.execute_reply.started":"2024-07-16T18:07:27.498196Z","shell.execute_reply":"2024-07-16T18:07:27.498211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n**activations** contain the outputs of all the 10 layers which can be plotted and visualized","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\nVisualize the activations of intermediate layers from layer 1 to 10","metadata":{}},{"cell_type":"code","source":"# Visualize the activations of the model for the specified image using the defined layer names\nshow_activations(activations, layer_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.499937Z","iopub.status.idle":"2024-07-16T18:07:27.500266Z","shell.execute_reply.started":"2024-07-16T18:07:27.500105Z","shell.execute_reply":"2024-07-16T18:07:27.500119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n* What we see in the above plots are the activations or the outputs of each of the 11 layers we chose \n* The activations or the outputs from the 1st layer(conv2d_1) don't lose much information of the original input\n* They are the results of applying several edge detecting filters on the input image**\n* With each added layer, the activations lose visual/input information and keeps building on the class/ouput information\n* As the depth increases, the layers activations become less visually interpretabale and more abstract\n* By doing so, they learn to detect more specific features of the class rather than just edges and curves\n* We plotted just 10 out of 314 intermediate layers. We already have in these few layers, activations which are blank/sparse(for ex: the 2 blank activations in the layer activation_1)\n* These blank/sparse activations are caused when any of the filters used in that layer didn't find a matching pattern in the input given to it\n* By plotting more layers(specially those towards the end of the network), we can observe more of these sparse activations and how the layers get more abstract\n","metadata":{}},{"cell_type":"markdown","source":"<a id=\"GettingtheActivationsforaDifferentInput/Food\"></a>\n# <b><span style='color:black'>Step 7.9 |</span><span style='color:#742d0c '> Getting the Activations for a Different Input / Food\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Get the activations for the specified image using the defined layer names\nactivations = get_activations(food, activations_output)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.501998Z","iopub.status.idle":"2024-07-16T18:07:27.502329Z","shell.execute_reply.started":"2024-07-16T18:07:27.502166Z","shell.execute_reply":"2024-07-16T18:07:27.502180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display the activations for the specified image using the defined layer names\nshow_activations(activations, layer_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.503968Z","iopub.status.idle":"2024-07-16T18:07:27.504296Z","shell.execute_reply.started":"2024-07-16T18:07:27.504137Z","shell.execute_reply":"2024-07-16T18:07:27.504151Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n\n* The feature maps in the above activations are for a different input image\n* We see the same patterns discussed for the previous input image\n* It is interesting to see the blank/sparse activations in the same layer(activation_1) and for same filters when a different image is passed to the network\n* Remember we used a pretrained Inceptionv3 model. All the filters that are used in different layers come from this pretrained model","metadata":{}},{"cell_type":"markdown","source":"<a id=\"LookintotheSparseActivationsintheLayerActivation_1/Food\"></a>\n# <b><span style='color:black'>Step 7.10 |</span><span style='color:#742d0c '> Look into the Sparse Activations in the Layer Activation_1\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* We have two blank/sparse activations in layer 6\n* Below cell displays one of the sparse activations","metadata":{}},{"cell_type":"code","source":"# Get the index of activation_1 layer which has sparse activations\nind = layer_names.index('activation_1')\nsparse_activation = activations[ind]\n# Select the activation values of a specific filter\na = sparse_activation[0, :, :, 13]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.505224Z","iopub.status.idle":"2024-07-16T18:07:27.505619Z","shell.execute_reply.started":"2024-07-16T18:07:27.505384Z","shell.execute_reply":"2024-07-16T18:07:27.505402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all(np.isnan(a[j][k]) for j in range(a.shape[0]) for k in range(a.shape[1]))\n#This line checks if all elements in the array a are NaN.","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.506813Z","iopub.status.idle":"2024-07-16T18:07:27.507112Z","shell.execute_reply.started":"2024-07-16T18:07:27.506961Z","shell.execute_reply":"2024-07-16T18:07:27.506973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* We can see that the activation has all nan values(it was all zeros when executed outside Kaggle, Im yet to figure out why its showing all nan values here\n \n* To know why we have all zero/nan values for this activation, lets visualize the activation at same index 13 from previous layer","metadata":{}},{"cell_type":"code","source":"# Get the index of batch_normalization_1 layer which has sparse activations\nind = layer_names.index('batch_normalization_1')\n# Extract sparse activations from the layer\nsparse_activation = activations[ind]\n# Select activations for the 14th filter\nb = sparse_activation[0, :, :, 13]\n# Print the sparse activations\nb","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.508024Z","iopub.status.idle":"2024-07-16T18:07:27.508327Z","shell.execute_reply.started":"2024-07-16T18:07:27.508175Z","shell.execute_reply":"2024-07-16T18:07:27.508188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* All the values in the above activation map from the layer batch_normalization_1 are negative\n* This activation in batch_normalization_1 is passed to the next layer activation_1 as input\n* As the name says, activation_1 is an activation layer and ReLu is the activation function used\n* ReLu takes an input value, returns 0 if its negative, the value otherwise\n* Since the input to activation array contains all negative values, the activation layer fills its activation map with all zeros for the index\n* Now we know why we have those 2 sparse activations in activation_1 layer","metadata":{}},{"cell_type":"markdown","source":"<a id=\"VisualizationofConvolutionalLayerActivations\"></a>\n# <b><span style='color:black'>Step 7.11 |</span><span style='color:#742d0c '> Visualization of Convolutional Layer Activations\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Extract activations for the first three convolutional layers\nfirst_convlayer_activation = activations[0]\nsecond_convlayer_activation = activations[3]\nthird_convlayer_activation = activations[6]\n\n# Visualize the activations for each layer\nf, ax = plt.subplots(1, 3, figsize=(10, 10))\n\n# Plot activations for the first convolutional layer\nax[0].imshow(first_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[0].axis('OFF')\nax[0].set_title('Conv2d_1')\n\n# Plot activations for the second convolutional layer\nax[1].imshow(second_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[1].axis('OFF')\nax[1].set_title('Conv2d_2')\n\n# Plot activations for the third convolutional layer\nax[2].imshow(third_convlayer_activation[0, :, :, 3], cmap='viridis')\nax[2].axis('OFF')\nax[2].set_title('Conv2d_3')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.509767Z","iopub.status.idle":"2024-07-16T18:07:27.510093Z","shell.execute_reply.started":"2024-07-16T18:07:27.509932Z","shell.execute_reply":"2024-07-16T18:07:27.509946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GeneratingClassActivationMap\"></a>\n# <b><span style='color:black'>Step 7.12 |</span><span style='color:#742d0c '> Generating Class Activation Map (CAM)\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"def get_attribution(food):\n    \"\"\"\n    Generate class activation map for a given food image using Grad-CAM technique.\n\n    Args:\n    food (str): Path to the food image.\n\n    Returns:\n    numpy.ndarray: Predictions made by the model for the input image.\n    \"\"\"\n    # Load and preprocess the input image\n    img = image.load_img(food, target_size=(299, 299))\n    img = image.img_to_array(img) \n    img /= 255. \n\n    # Display the input image\n    f, ax = plt.subplots(1, 3, figsize=(15, 15))\n    ax[0].imshow(img)\n    ax[0].set_title(\"Input Image\")\n\n    # Expand the dimensions and predict the class probabilities\n    img = np.expand_dims(img, axis=0) \n    preds = model.predict(img)\n    class_id = np.argmax(preds[0])\n\n    # Get the class output and last convolutional layer\n    class_output = model.output[:, class_id]\n    last_conv_layer = model.get_layer(\"mixed10\")\n    \n    # Calculate gradients and pooled gradients\n    grads = K.gradients(class_output, last_conv_layer.output)[0]\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([img])\n\n    # Generate heatmap\n    for i in range(2048):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap /= np.max(heatmap)\n    ax[1].imshow(heatmap)\n    ax[1].set_title(\"Heat map\")\n    \n    # Overlay heatmap on the original image\n    act_img = cv2.imread(food)\n    heatmap = cv2.resize(heatmap, (act_img.shape[1], act_img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    superimposed = cv2.addWeighted(act_img, 0.6, heatmap, 0.4, 0)\n    cv2.imwrite('classactivation.png', superimposed)\n    img_act = image.load_img('classactivation.png', target_size=(299, 299))\n    ax[2].imshow(img_act)\n    ax[2].set_title(\"Class Activation\")\n    plt.show()\n    return preds\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.511674Z","iopub.status.idle":"2024-07-16T18:07:27.512027Z","shell.execute_reply.started":"2024-07-16T18:07:27.511845Z","shell.execute_reply":"2024-07-16T18:07:27.511859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Showing the class map..\")\nprint(class_map_3)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.513389Z","iopub.status.idle":"2024-07-16T18:07:27.513755Z","shell.execute_reply.started":"2024-07-16T18:07:27.513590Z","shell.execute_reply":"2024-07-16T18:07:27.513604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"GettingAttributionandDisplaySoftmaxPredictions\"></a>\n# <b><span style='color:black'>Step 7.13 |</span><span style='color:#742d0c '> Getting Attribution and Display Softmax Predictions\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)\n","metadata":{}},{"cell_type":"code","source":"# Get attribution for the image 'applepie.jpg' and display the softmax predictions\npred = get_attribution('applepie.jpg')\nprint(\"Here are softmax predictions:\", pred)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.514978Z","iopub.status.idle":"2024-07-16T18:07:27.515308Z","shell.execute_reply.started":"2024-07-16T18:07:27.515145Z","shell.execute_reply":"2024-07-16T18:07:27.515159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get attribution and display softmax predictions for the pizza image\npred = get_attribution('pizza.jpg')\nprint(\"Here are softmax predictions:\", pred)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.516376Z","iopub.status.idle":"2024-07-16T18:07:27.516719Z","shell.execute_reply.started":"2024-07-16T18:07:27.516551Z","shell.execute_reply":"2024-07-16T18:07:27.516565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:125%; text-align:left\">\n    \n* We can see how the heat map is different for a different image i.e the model looks for a totally different features/regions if it has to classify it as a pizza","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n- Lets see if we can break the model or see what it does when we surpise it with different data!","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n* We trained our model to perform multi class classification and it seems to be doing well with >95% of accuracy\n* What will the model do when we give it an image which has more than one object that model is trained to classify?","metadata":{}},{"cell_type":"markdown","source":"<a id=\"DownloadingImagesfromURLs\"></a>\n# <b><span style='color:black'>Step 7.14 |</span><span style='color:#742d0c '> Downloading Images from URLs\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"# Downloading images from internet using the URLs\n!wget -O piepizza.jpg https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/piepizza.jpg\n!wget -O piepizzas.png https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/piepizzas.png\n!wget -O pizzapie.jpg https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/pizzapie.jpg\n!wget -O pizzapies.png https://raw.githubusercontent.com/theimgclist/PracticeGround/master/Food101/pizzapies.png\n","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.517898Z","iopub.status.idle":"2024-07-16T18:07:27.518197Z","shell.execute_reply.started":"2024-07-16T18:07:27.518046Z","shell.execute_reply":"2024-07-16T18:07:27.518058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"LoadingandRetrievingActivations\"></a>\n# <b><span style='color:black'>Step 7.15 |</span><span style='color:#742d0c '> Loading and Retrieving Activations\n\n</span></b>\n⬆️ [Tabel of Contents](#contents_tabel)","metadata":{}},{"cell_type":"code","source":"food = 'piepizza.jpg'\nactivations = get_activations(food,activations_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.520005Z","iopub.status.idle":"2024-07-16T18:07:27.520307Z","shell.execute_reply.started":"2024-07-16T18:07:27.520157Z","shell.execute_reply":"2024-07-16T18:07:27.520170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_activations(activations, layer_names)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.521565Z","iopub.status.idle":"2024-07-16T18:07:27.521879Z","shell.execute_reply.started":"2024-07-16T18:07:27.521726Z","shell.execute_reply":"2024-07-16T18:07:27.521739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('piepizza.jpg')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.523425Z","iopub.status.idle":"2024-07-16T18:07:27.523775Z","shell.execute_reply.started":"2024-07-16T18:07:27.523609Z","shell.execute_reply":"2024-07-16T18:07:27.523623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n\n* Given an image with pizza and applepie, the model thinks its a pizza with 75.4% confidence and an applie pie with 18% confidence\n* Now let's flip the image vertically and see what the model does","metadata":{}},{"cell_type":"code","source":"food = 'pizzapie.jpg'\nactivations = get_activations(food,activations_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.525159Z","iopub.status.idle":"2024-07-16T18:07:27.525502Z","shell.execute_reply.started":"2024-07-16T18:07:27.525323Z","shell.execute_reply":"2024-07-16T18:07:27.525337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('pizzapie.jpg')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.526640Z","iopub.status.idle":"2024-07-16T18:07:27.526943Z","shell.execute_reply.started":"2024-07-16T18:07:27.526793Z","shell.execute_reply":"2024-07-16T18:07:27.526806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n\n* Well, the model flipped its output too!\n* The model now thinks its an apple pie with 49.7% confidence and a pizza with 31.9%","metadata":{}},{"cell_type":"markdown","source":"<h2 align=\"left\"><font color=#ed2323>More surprise data to the model...</font></h2>\n","metadata":{}},{"cell_type":"code","source":"food = 'pizzapies.png'\nactivations = get_activations(food,activations_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.528326Z","iopub.status.idle":"2024-07-16T18:07:27.528662Z","shell.execute_reply.started":"2024-07-16T18:07:27.528474Z","shell.execute_reply":"2024-07-16T18:07:27.528509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('pizzapies.png')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.529636Z","iopub.status.idle":"2024-07-16T18:07:27.529965Z","shell.execute_reply.started":"2024-07-16T18:07:27.529805Z","shell.execute_reply":"2024-07-16T18:07:27.529819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n\n* This time it's applie pie with 73% and a pizza with 19% confidence\n* Let's try one last horizontal flip, this is the last really!","metadata":{}},{"cell_type":"code","source":"food = 'piepizzas.png'\nactivations = get_activations(food,activations_output)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.531267Z","iopub.status.idle":"2024-07-16T18:07:27.531624Z","shell.execute_reply.started":"2024-07-16T18:07:27.531428Z","shell.execute_reply":"2024-07-16T18:07:27.531442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = get_attribution('piepizzas.png')\nprint(\"Here are softmax predictions..\",pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T18:07:27.532875Z","iopub.status.idle":"2024-07-16T18:07:27.533194Z","shell.execute_reply.started":"2024-07-16T18:07:27.533038Z","shell.execute_reply":"2024-07-16T18:07:27.533052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n* No surprise from model this time. We flipped the image but the model didnt flip its output\n* It's an apple pie again with 52% confidence","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n### Further Improvements\n* Try more augmentation on test images\n* Fine tune the model on the entire dataset(for a few epochs atleast)\n* Play with hyper parameters, their values and see how it impacts model performance\n* There is currently no implementation to handle out of distribution / no class scenario. Can try below methods:\n    \n  * Set a threshold for the class with highest score. When model gives prediction score below the threshold for its top prediction, the prediction can be classified as NO-CLASS / UNSEEN\n  * Add a new class called NO-CLASS, provide data from different classes other than those in the original dataset. This way the model also learns how to classify totally unseen/unrelated data\n  * I am yet to try these methods and not sure about the results\n* Recently published paper - [Rethinking ImageNet Pretraining](https://arxiv.org/abs/1811.08883 ), claims that training from random initialization instead of using pretrained weights is not only robust but also gives comparable results\n* Pre-trained models are surely helpful. They save a lot of time and computation. Yet, that shouldn't be the reason to not try to train a model from scratch\n* Time taking yet productive experiment would be to try and train a model on this dataset from scratch\n* Do more experiments with Model Interpretability and see what can be observed","metadata":{}},{"cell_type":"markdown","source":"<div style=\"border-radius:10px; padding: 15px; background-color:gainsboro; font-size:150%; text-align:left\">\n    \n### Feedback\n* Did you find any issues with the above code or have any suggestions or corrections?**\n* There must be many ways to improve the model, its architecture, hyperparameters..**\n* Please do let me know!\n* [Github](https://github.com/Engr-Umer)\n* [Linkedin](https://www.linkedin.com/in/muhammad-umer-mujahid/)","metadata":{}},{"cell_type":"markdown","source":"<h1 align=\"left\"><font color=#ed2323>Love you all and keep supporting</font></h1>","metadata":{}}]}